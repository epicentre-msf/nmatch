% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nmatch_fast.R
\name{nmatch_fast}
\alias{nmatch_fast}
\title{Compare sets of proper names accounting for common types of variation in
format and style, optimized using Rcpp}
\usage{
nmatch_fast(
  x,
  y,
  token_split = "[-_[:space:]]+",
  nchar_min = 2L,
  std = name_standardize,
  ...
)
}
\arguments{
\item{x, y}{Vectors of proper names to compare. Must be of same length.}

\item{token_split}{Regex pattern to split strings into tokens. Defaults to
\code{"[-_[:space:]]+"}, which splits at each sequence of one more dash,
underscore, or space character.}

\item{nchar_min}{Minimum token size to compare. Defaults to \code{2L}.}

\item{std}{Function to standardize strings during matching. Defaults to
\code{\link{name_standardize}}. Set to \code{NULL} to omit standardization.}

\item{...}{additional arguments passed to \code{std()}}
}
\value{
Returns an integer matrix summarizing the match details, including columns:
\itemize{
\item \code{is_match}: logical vector indicating overall match status
\item \code{k_x}: number of tokens in \code{x} (excludes tokens smaller than \code{nchar_min})
\item \code{k_y}: number of tokens in \code{y} (excludes tokens smaller than \code{nchar_min})
\item \code{k_align}: number of aligned tokens (i.e. \code{min(k_x, k_y)})
\item \code{n_match}: number of aligned tokens that match (i.e. distance <= \code{dist_max})
\item \code{dist_total}: summed string distance across aligned tokens
}
}
\description{
Compare proper names across two sources using string-standardization to
account for variation in punctuation, accents, and character case,
token-permutation to account for variation in name order, and fuzzy matching
to handle alternate spellings. The specific steps are:
\enumerate{
\item Standardize strings. The default function is
\code{\link{name_standardize}} which removes accents and punctuation,
standardizes case, and removes extra whitespace. E.g. "Brontë, Emily J." is
standardized to "BRONTE EMILY J".
\item Tokenize standardized names, optionally retaining only tokens larger than
a given nchar limit.
\item For each pair of names, calculate string distance between all combinations
of tokens, and find the best overall token alignment (i.e. the alignment that
minimizes the summed string distance). If two names being compared differ in
their number of tokens, the alignment is made with respect to the smaller
number of tokens. E.g. If comparing "Angela Dorothea Merkel" to "Merkel
Angela", the token "Dorothea" would ultimately be omitted from the best
alignment.
}
}
\examples{
names1 <- c(
  "Angela Dorothea Merkel",
  "Emmanuel Jean-Michel Fr\u00e9d\u00e9ric Macron",
  "Mette Frederiksen",
  "Katrin Jakobsd\u00f3ttir",
  "Pedro S\u00e1nchez P\u00e9rez-Castej\u00f3n"
)

names2 <- c(
  "MERKEL, Angela",
  "MACRON, Emmanuel J.-M. F.",
  "FREDERICKSON, Mette",
  "JAKOBSDOTTIR  Kathríne",
  "PEREZ-CASTLEJON, Pedro"
)

# return logical vector specifying which names are matches
nmatch_fast(names1, names2)

}
